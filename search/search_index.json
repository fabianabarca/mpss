{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Modelos Probabil\u00edsticos de Se\u00f1ales y Sistemas","text":"<p>Estas son notas de clase del curso, acompa\u00f1ados de ejercicios de la teor\u00eda y de programaci\u00f3n.</p>"},{"location":"#asignaciones","title":"Asignaciones","text":"<ol> <li> <p>(2%) \u00a1Repartir estrellas! Visitar GitHub con su cuenta (la misma que ser\u00e1 usada en el proyecto) y colocar una estrella en los siguientes repositorios relacionados con el curso:</p> mpss <p>Este repositorio de notas de clase</p> pyx <p>Tutoriales introductorios de Python y sus herramientas para an\u00e1lisis de datos</p> kalouk <p>Presentaciones web para matem\u00e1ticas y programaci\u00f3n, desarrollado para este curso</p> slidev-theme-kalouk <p>Tema visual de Kalouk para Slidev, la plataforma de presentaciones web base</p> slidev-addon-kalouk <p>Funcionalidades de Kalouk para Slidev, la plataforma de presentaciones web base</p> </li> <li> <p>(8%) Realizar una transcripci\u00f3n de las presentaciones seg\u00fan la distribuci\u00f3n de personas y documentos de la tabla siguiente.</p> </li> </ol> <p>\u00bfQu\u00e9 debo transcribir y c\u00f3mo?</p> <p>En cada documento dentro de <code>docs/</code> hay una indicaci\u00f3n de las secciones espec\u00edficas para transcribir. Pueden ver una explicaci\u00f3n varios ejemplos en las notas sobre la transcripci\u00f3n.</p> <p>Para saber c\u00f3mo ejecutar localmente el repositorio, c\u00f3mo ver en su navegador esta documentaci\u00f3n y c\u00f3mo editar en un ambiente integrado de desarrollo (VS Code, por ejemplo), pueden consultar el documento HOWTO con las instrucciones b\u00e1sicas.</p>"},{"location":"#asignaciones-de-documentos","title":"Asignaciones de documentos","text":"<p>Actualmente cada documento tiene un contenido como:</p> <pre><code>### Presentaci\u00f3n\n\n[0 - Teor\u00eda de conjuntos y an\u00e1lisis combinatorio](https://www.overleaf.com/read/rdsdsqffjcwc#f222aa)\n\n### Secciones\n\n- Teor\u00eda de conjuntos (1 - 19)\n</code></pre> <p>donde indica cu\u00e1l presentaci\u00f3n corresponde (ver siguiente secci\u00f3n) y cu\u00e1les secciones espec\u00edficas.</p> Apellido Nombre Documento (link) Elizondo Castrillo Arianna Victoria <code>1_0_1_conjuntos.md</code> Villalobos Vega Ra\u00fal Andr\u00e9s <code>1_0_2_combinatorio.md</code> Mart\u00ednez Jim\u00e9nez Kenneth Josu\u00e9 <code>1_1_1_la_probabilidad.md</code> Vega Ure\u00f1a Yendry Patricia <code>1_1_2_notas.md</code> L\u00f3pez L\u00f3pez Sebasti\u00e1n <code>1_2_1_conjunta_total.md</code> Arce Arias Abraham Isaac <code>1_2_2_bayes.md</code> Acosta Mendoza Luis Fernando <code>1_3_1_eventos_independientes.md</code> Vargas Jarqu\u00edn Nicole <code>1_3_2_pruebas_bernoulli.md</code> Hurtado Ram\u00edrez Jerry Adolfo <code>2_4_1_variables_aleatorias.md</code> Mora Porras Bryan <code>2_4_2_funciones_distribucion_acumulativa.md</code> Arroyo S\u00e1nchez Charlie Jos\u00e9 <code>2_4_3_funciones_probabilidad.md</code> Feng Wu Evelyn <code>2_4_4_funciones_aplicacion_comun.md</code> Rojas \u00c1lvarez Cristhian Estuard <code>2_5_1_funcion_acumulativa_condicional.md</code> Le\u00f3n Solano Jos\u00e9 Armando <code>2_5_2_casos_especiales.md</code> Rojas Rojas Oscar Francisco <code>2_5_3_funcion_densidad_condicional.md</code> Fuentes C\u00f3rdoba Oscar Alfonso <code>2_6_1_valor_esperado_va.md</code> Arias Abarca Oscar Santiago <code>2_6_2_momentos_va.md</code> Vado Chavarr\u00eda Wendolyn Maguel <code>2_7_1_generadora_caracteristica.md</code> Gonz\u00e1lez G\u00e1mez Jacob <code>2_7_2_ejemplos.md</code> Madrigal Chaves Andr\u00e9 <code>2_7_3_ejemplos_de_determinacion.md</code> Carmona Zelaya Josu\u00e9 Dar\u00edo <code>2_7_4_ejemplos_de_determinacion.md</code> Altamiranda Solano Christian <code>2_8_1_disipacion_potencia.md</code> Quir\u00f3s Olivares Aylin Denise <code>2_8_2_transformaciones_va.md</code> \u00c1lvarez Sand\u00ed Joseph Alberto <code>2_8_3_no_monotonicas.md</code> Mena Porras Nicole Andre\u00edna <code>3_10_1_valor_esperado_momentos_correlacion.md</code> God\u00ednez Duran Alejandro <code>3_10_2_independencia_ortogonalidad.md</code> Canales Salvatierra Jeaustin David <code>3_11_1_transformacion_va_multiples.md</code> Wolfe Clayton Kevin Ulises <code>3_11_2_vida_util_y_distribucion.md</code> Montero Campos Cristopher David <code>3_12_1_limite_central_sumas.md</code> Jim\u00e9nez Salazar Nathaly <code>3_12_2_limite_central_muestras.md</code> Gonz\u00e1lez Mora Rams\u00e9s El\u00edas <code>3_12_3_desigualdad_chebyshev.md</code> Fonseca Obando Rachel Berenizce <code>3_12_4_desigualdad_markov.md</code> Jim\u00e9nez Ortega Gerardo Alejandro <code>3_9_1_variable_aleatoria_multiple.md</code> Corrales Duarte Esteban <code>3_9_2_propiedades_acumulativa.md</code> Fallas Jim\u00e9nez Mart\u00edn Leandro <code>3_9_3_propiedades_densidad_conjunta.md</code> Escobar Arrieta Guillermo <code>3_9_4_independencia_estadistica.md</code> Aguilar Chac\u00f3n Carlos Ramiro <code>4_13_1_concepto_estocastico.md</code> Arias Fallas Caleb <code>4_13_2_clasificacion.md</code> Arag\u00f3n Herrera Esteban Andr\u00e9s <code>4_13_3_funciones_distribucion.md</code> Guill\u00e9n Cordero Karina <code>4_13_4_estacionaridad.md</code> Rojas Guti\u00e9rrez Sebasti\u00e1n <code>4_14_1_ergodicidad.md</code> Garro Arias Danny Andr\u00e9s <code>4_14_2_correlacion_covarianza.md</code> Ruiz Mart\u00ednez Aar\u00f3n Andr\u00e9s <code>4_14_3_funciones_covarianza.md</code> Sol\u00eds Gonz\u00e1lez Fanny Mariana <code>4_15_1_espectro_densidad.md</code> V\u00edquez Solano Daniel <code>4_15_2_propiedades.md</code> Fallas Guti\u00e9rrez C\u00e9sar Luis <code>4_16_1_respuesta.md</code> Obando Picado Joan Josu\u00e9 <code>4_16_2_autocorrelacion_caracteristicas.md</code> Sierra Berrocal Alfredo Auriel <code>5_17_1_proceso_funciones_poisson.md</code> Cruz Meza Zo\u00e9 Deliana <code>5_17_2_momento_densidad.md</code> Montero Calder\u00f3n Jean Paul <code>5_18_1_definicion_markov.md</code> Rojas Mor\u00faa Luis Fernando <code>5_18_2_nacimiento_muerte.md</code> S\u00e1enz D\u00edaz Jos\u00e9 Ignacio <code>5_18_3_teoria_de_colas.md</code> Rosales S\u00e1nchez Mariana <code>5_19_1_vector_estable.md</code> Vargas Arce Camila Amelia <code>5_19_2_cola_servidores.md</code> Jim\u00e9nez Mata Cristhoper <code>5_19_3_sistema_colas_mms.md</code> Coto Portuguez Jos\u00e9 Pablo <code>5_20_1_markov_discreto.md</code> Morales Salazar Monserrat <code>5_20_2_markov_discreto.md</code>"},{"location":"#presentaciones-en-overleaf","title":"Presentaciones en Overleaf","text":"<p>Link externo a la presentaci\u00f3n en LaTeX (modo lectura).</p> Nombre 0 - Teor\u00eda de conjuntos y an\u00e1lisis combinatorio 1 - La probabilidad 2 - Probabilidad conjunta, condicional y teorema de Bayes 3 - Eventos independientes y pruebas de Bernoulli 4 - Variables aleatorias 5 - Funciones de distribuci\u00f3n condicionales 6 - Momentos de una variable aleatoria 7 - Funciones que dan momentos 8 - Transformaciones de una variable aleatoria 9 - Variables aleatorias m\u00faltiples 10 - Momentos de variables aleatorias m\u00faltiples 11 - Transformaciones de variables aleatorias m\u00faltiples 12 - Teorema del l\u00edmite central y otros 13 - Procesos estoc\u00e1sticos 14 - Ergodicidad y funciones de correlaci\u00f3n 15 - Caracter\u00edsticas espectrales de procesos estoc\u00e1sticos 16 - Respuesta de sistemas lineales a una se\u00f1al aleatoria 17 - Proceso contador de Poisson 18 - Cadenas de Markov de tiempo continuo 19 - Markov de tiempo continuo y el vector de estado estable 20 - Cadenas de Markov de tiempo discreto 21 - Markov de tiempo discreto y el vector de estado estable"},{"location":"1_0_1_conjuntos/","title":"Teor\u00eda de conjuntos","text":""},{"location":"1_0_1_conjuntos/#presentacion","title":"Presentaci\u00f3n","text":"<p>0 - Teor\u00eda de conjuntos y an\u00e1lisis combinatorio</p>"},{"location":"1_0_1_conjuntos/#secciones","title":"Secciones","text":"<ul> <li>Teor\u00eda de conjuntos (1 - 19)</li> </ul>"},{"location":"1_0_2_combinatorio/","title":"An\u00e1lisis combinatorio","text":""},{"location":"1_0_2_combinatorio/#presentacion","title":"Presentaci\u00f3n","text":"<p>0 - Teor\u00eda de conjuntos y an\u00e1lisis combinatorio</p>"},{"location":"1_0_2_combinatorio/#secciones","title":"Secciones","text":"<ul> <li>Algunos resultados \u00fatiles del an\u00e1lisis combinatorio (20 - 30)</li> </ul>"},{"location":"1_0_introduccion/","title":"Introducci\u00f3n","text":"<p>La probabilidad es una rama de la matem\u00e1tica con inmensa aplicaci\u00f3n pr\u00e1ctica en muchas disciplinas: desde el quehacer personal y dom\u00e9stico hasta grandes decisiones sociales. En nuestra disciplina es, adem\u00e1s, fundamental para el an\u00e1lisis de se\u00f1ales y sistemas.</p>"},{"location":"1_1_1_la_probabilidad/","title":"\u00bfQu\u00e9 es y para qu\u00e9 sirve la probabilidad?","text":""},{"location":"1_1_1_la_probabilidad/#presentacion","title":"Presentaci\u00f3n","text":"<p>1 - La probabilidad</p>"},{"location":"1_1_1_la_probabilidad/#secciones","title":"Secciones","text":"<ul> <li>\u00bfQu\u00e9 es y para qu\u00e9 sirve la probabilidad? (1 - 14)</li> </ul>"},{"location":"1_1_2_Notas/","title":"1 1 2 Notas","text":""},{"location":"1_1_2_Notas/#presentacion","title":"Presentaci\u00f3n","text":"<p>1 - La probabilidad</p>"},{"location":"1_1_2_Notas/#secciones","title":"Secciones","text":"<ul> <li>Notas (15 - 27)</li> </ul>"},{"location":"1_2_1_conjunta_total/","title":"Probabilidad conjunta y condicional","text":""},{"location":"1_2_1_conjunta_total/#presentacion","title":"Presentaci\u00f3n","text":"<p>2 - Probabilidad conjunta, condicional y teorema de Bayes</p>"},{"location":"1_2_1_conjunta_total/#secciones","title":"Secciones","text":"<ul> <li>Introducci\u00f3n, probabilidad conjunta y probabilidad total (1 - 15)</li> </ul>"},{"location":"1_2_2_bayes/","title":"Teorema de Bayes","text":""},{"location":"1_2_2_bayes/#presentacion","title":"Presentaci\u00f3n","text":"<p>2 - Probabilidad conjunta, condicional y teorema de bayes</p>"},{"location":"1_2_2_bayes/#secciones","title":"Secciones","text":"<ul> <li>Teorema de Bayes (16 - 32)</li> </ul>"},{"location":"1_3_1_eventos_Independientes/","title":"1 3 1 eventos Independientes","text":""},{"location":"1_3_1_eventos_Independientes/#presentacion","title":"Presentaci\u00f3n","text":"<p>3 - Eventos independientes y pruebas de Bernoulli</p>"},{"location":"1_3_1_eventos_Independientes/#secciones","title":"Secciones","text":"<ul> <li>Eventos independientes (1 - 17)</li> </ul>"},{"location":"1_3_2_pruebas_bernoulli/","title":"Pruebas repetidas de Bernoulli","text":""},{"location":"1_3_2_pruebas_bernoulli/#presentacion","title":"Presentaci\u00f3n","text":"<p>3 - Eventos independientes y pruebas de Bernoulli</p>"},{"location":"1_3_2_pruebas_bernoulli/#secciones","title":"Secciones","text":"<ul> <li>Pruebas de Bernoulli (18 - 27)</li> </ul>"},{"location":"2_0_introduccion/","title":"Introducci\u00f3n","text":"<p>Las variables aleatorias facilitan una manipulaci\u00f3n num\u00e9rica m\u00e1s robusta de los fen\u00f3menos aleatorios, y permiten extender el an\u00e1lisis a muchos m\u00e1s casos que los vistos hasta ahora. Herramientas como las funciones de densidad y de distribuci\u00f3n acumulativa de probabilidad proveen descripciones completas de los modelos probabil\u00edsticos.</p>"},{"location":"2_4_1_variables_aleatorias/","title":"Definiciones","text":""},{"location":"2_4_1_variables_aleatorias/#presentacion","title":"Presentaci\u00f3n","text":"<p>4 - Variables aleatorias</p>"},{"location":"2_4_1_variables_aleatorias/#secciones","title":"Secciones","text":"<ul> <li>Variables aleatorias (1 - 12)</li> </ul>"},{"location":"2_4_2_funciones_distribucion_acumulativa/","title":"Funci\u00f3n de probabilidad acumulativa","text":""},{"location":"2_4_2_funciones_distribucion_acumulativa/#presentacion","title":"Presentaci\u00f3n","text":"<p>4 - Variables aleatorias</p>"},{"location":"2_4_2_funciones_distribucion_acumulativa/#secciones","title":"Secciones","text":"<ul> <li>Funci\u00f3n de probabilidad acumulativa (13 - 24)</li> </ul>"},{"location":"2_4_3_funciones_probabilidad/","title":"Funci\u00f3n de densidad de probabilidad","text":""},{"location":"2_4_3_funciones_probabilidad/#presentacion","title":"Presentaci\u00f3n","text":"<p>4 - Variables aleatorias</p>"},{"location":"2_4_3_funciones_probabilidad/#secciones","title":"Secciones","text":"<ul> <li>Funci\u00f3n de densidad de probabilidad (25 - 31)</li> </ul>"},{"location":"2_4_4_funciones_aplicacion_comun/","title":"Algunas distribuciones comunes","text":""},{"location":"2_4_4_funciones_aplicacion_comun/#presentacion","title":"Presentaci\u00f3n","text":"<p>4 - Variables aleatorias</p>"},{"location":"2_4_4_funciones_aplicacion_comun/#secciones","title":"Secciones","text":"<ul> <li>Algunas funciones de distribuci\u00f3n probabil\u00edstica de aplicaci\u00f3n com\u00fan (32 - 48)</li> </ul>"},{"location":"2_5_1_funcion_acumulativa_condicional/","title":"Funci\u00f3n acumulativa condicional","text":""},{"location":"2_5_1_funcion_acumulativa_condicional/#presentacion","title":"Presentaci\u00f3n","text":"<p>5 - Funciones de distribuci\u00f3n condicionales</p>"},{"location":"2_5_1_funcion_acumulativa_condicional/#secciones","title":"Secciones","text":"<ul> <li>Funci\u00f3n acumulativa condicional (1 - 9)</li> </ul>"},{"location":"2_5_2_casos_especiales/","title":"Casos especiales de la funci\u00f3n acumulativa y sus ejemplos","text":""},{"location":"2_5_2_casos_especiales/#presentacion","title":"Presentaci\u00f3n","text":"<p>5 - Funciones de distribuci\u00f3n condicionales</p>"},{"location":"2_5_2_casos_especiales/#secciones","title":"Secciones","text":"<ul> <li>Casos especiales de la funci\u00f3n acumulativa y sus ejemplos (10 - 21)</li> </ul>"},{"location":"2_5_3_funcion_densidad_condicional/","title":"Funci\u00f3n de densidad condicional","text":""},{"location":"2_5_3_funcion_densidad_condicional/#presentacion","title":"Presentaci\u00f3n","text":"<p>5 - Funciones de distribuci\u00f3n condicionales</p>"},{"location":"2_5_3_funcion_densidad_condicional/#secciones","title":"Secciones","text":"<ul> <li>Funci\u00f3n de densidad condicional (22 - 25)</li> </ul>"},{"location":"2_6_1_valor_esperado_va/","title":"Valor esperado","text":""},{"location":"2_6_1_valor_esperado_va/#presentacion","title":"Presentaci\u00f3n","text":"<p>6 - Valor esperado y momento de una variable aleatoria</p>"},{"location":"2_6_1_valor_esperado_va/#secciones","title":"Secciones","text":"<ul> <li>Valor esperado de una variable aleatoria (1 - 13)</li> </ul>"},{"location":"2_6_2_momentos_va/","title":"Momentos","text":""},{"location":"2_6_2_momentos_va/#presentacion","title":"Presentaci\u00f3n","text":"<p>6 - Valor esperado y momento de una variable aleatoria</p>"},{"location":"2_6_2_momentos_va/#secciones","title":"Secciones","text":"<ul> <li>Momentos de una variable aleatoria (14 - 28)</li> </ul>"},{"location":"2_7_1_generadora_caracteristica/","title":"Funci\u00f3n generadora de momentos y funci\u00f3n caracter\u00edstica","text":""},{"location":"2_7_1_generadora_caracteristica/#presentacion","title":"Presentaci\u00f3n","text":"<p>7 - Funciones que dan momentos</p>"},{"location":"2_7_1_generadora_caracteristica/#secciones","title":"Secciones","text":"<ul> <li>Funci\u00f3n generadora de momentos (1 - 6)</li> <li>Funci\u00f3n caracter\u00edstica (7-9)</li> </ul>"},{"location":"2_7_2_ejemplos/","title":"Ejemplos para diferentes funciones","text":""},{"location":"2_7_2_ejemplos/#presentacion","title":"Presentaci\u00f3n","text":"<p>7 - Funciones que dan momentos</p>"},{"location":"2_7_2_ejemplos/#secciones","title":"Secciones","text":"<ul> <li>Ejemplos para diferentes funciones (10 - 17)</li> </ul>"},{"location":"2_7_3_ejemplos_de_determinacion/","title":"Ejemplo de determinaci\u00f3n de la funci\u00f3n caracter\u00edstica I-VII","text":""},{"location":"2_7_3_ejemplos_de_determinacion/#presentacion","title":"Presentaci\u00f3n","text":"<p>7 - Funciones que dan momentos</p>"},{"location":"2_7_3_ejemplos_de_determinacion/#secciones","title":"Secciones","text":"<ul> <li>Ejemplo de determinaci\u00f3n de la funci\u00f3n caracter\u00edstica I-VII (18 - 24)</li> </ul>"},{"location":"2_7_4_ejemplos_de_determinacion/","title":"Ejemplo de determinaci\u00f3n de la funci\u00f3n caracter\u00edstica VIII-XII","text":""},{"location":"2_7_4_ejemplos_de_determinacion/#presentacion","title":"Presentaci\u00f3n","text":"<p>7 - Funciones que dan momentos</p>"},{"location":"2_7_4_ejemplos_de_determinacion/#secciones","title":"Secciones","text":"<ul> <li>Ejemplo de determinaci\u00f3n de la funci\u00f3n caracter\u00edstica VIII-XII (18 - 24)</li> </ul>"},{"location":"2_8_1_disipacion_potencia/","title":"Ejemplos de disipaci\u00f3n de potencia y de una variable aleatoria","text":""},{"location":"2_8_1_disipacion_potencia/#presentacion","title":"Presentaci\u00f3n","text":"<p>8 - Transformaciones de una variable aleatoria</p>"},{"location":"2_8_1_disipacion_potencia/#secciones","title":"Secciones","text":"<ul> <li>Ejemplos de disipaci\u00f3n de potencia (14 - 23)</li> <li>Ejemplo de una variable aleatoria, TI, TII y TIII (24 - 28)</li> </ul>"},{"location":"2_8_2_transformaciones_va/","title":"Transformaciones de una variable aleatoria","text":""},{"location":"2_8_2_transformaciones_va/#presentacion","title":"Presentaci\u00f3n","text":"<p>8 - Transformaciones de una variable aleatoria</p>"},{"location":"2_8_2_transformaciones_va/#secciones","title":"Secciones","text":"<ul> <li>Transformaciones de una variable aleatoria (1 - 13)</li> </ul>"},{"location":"2_8_3_no_monotonicas/","title":"Transformaciones no monot\u00f3nicas y sus ejemplos","text":""},{"location":"2_8_3_no_monotonicas/#presentacion","title":"Presentaci\u00f3n","text":"<p>8 - Transformaciones de una variable aleatoria</p>"},{"location":"2_8_3_no_monotonicas/#secciones","title":"Secciones","text":"<ul> <li>Transformaciones no monot\u00f3nicas de una va y sus ejemplos (29 - 40)</li> </ul>"},{"location":"3_0_introduccion/","title":"Introducci\u00f3n","text":"<p>En muchos experimentos, las observaciones no son una sola cantidad, sino un grupo o familia de cantidades. Por tanto, se construyen variables aleatorias m\u00faltiples, llamados vectores aleatorios.</p>"},{"location":"3_10_1_valor_esperado_momentos_correlacion/","title":"Valor esperado, momentos conjuntos alrededor del origen y correlaci\u00f3n","text":""},{"location":"3_10_1_valor_esperado_momentos_correlacion/#presentacion","title":"Presentaci\u00f3n","text":"<p>10 - Momentos de las variables aleatorias m\u00faltiples</p>"},{"location":"3_10_1_valor_esperado_momentos_correlacion/#secciones","title":"Secciones","text":"<ul> <li>Valor esperado de una funci\u00f3n de variables aleatorias (1 - 3)</li> <li>Momentos conjuntos alrededor del origen (4 - 5)</li> <li>Correlaci\u00f3n (6 - 9)</li> </ul>"},{"location":"3_10_2_independencia_ortogonalidad/","title":"Independencia, ortogonalidad y momentos centrales conjuntos","text":""},{"location":"3_10_2_independencia_ortogonalidad/#presentacion","title":"Presentaci\u00f3n","text":"<p>10 - Momentos de las variables aleatorias m\u00faltiples</p>"},{"location":"3_10_2_independencia_ortogonalidad/#secciones","title":"Secciones","text":"<ul> <li>Independencia y ortogonalidad y sus ejemplos (10-15)</li> <li>Momentos centrales conjuntos (16 - 22)</li> </ul>"},{"location":"3_11_1_transformacion_va_multiples/","title":"Transformaciones de variables aleatorias m\u00faltiples","text":""},{"location":"3_11_1_transformacion_va_multiples/#presentacion","title":"Presentaci\u00f3n","text":"<p>11 - Transformaciones de variables aleatorias m\u00faltiples</p>"},{"location":"3_11_1_transformacion_va_multiples/#secciones","title":"Secciones","text":"<ul> <li>Transformaciones de variables aleatorias m\u00faltiples (1 - 11)</li> </ul>"},{"location":"3_11_2_vida_util_y_distribucion/","title":"Vida \u00fatil de un componente y distribuci\u00f3n probabil\u00edstica","text":""},{"location":"3_11_2_vida_util_y_distribucion/#presentacion","title":"Presentaci\u00f3n","text":"<p>11 - Transformaciones de variables aleatorias m\u00faltiples</p>"},{"location":"3_11_2_vida_util_y_distribucion/#secciones","title":"Secciones","text":"<ul> <li>Vida \u00fatil de un componente y su repuesto I-VI (12 - 20)</li> <li>Distribuci\u00f3n probabil\u00edstica de la suma de variables aleatorias (21 - 29)</li> </ul>"},{"location":"3_12_1_limite_central_sumas/","title":"Teorema del l\u00edmite central para sumas","text":""},{"location":"3_12_1_limite_central_sumas/#presentacion","title":"Presentaci\u00f3n","text":"<p>12 - Teorema del l\u00edmite central: Desigualdades de Chebyshev y Markov y ley de los grandes n\u00fameros</p>"},{"location":"3_12_1_limite_central_sumas/#secciones","title":"Secciones","text":"<ul> <li>Teorema del l\u00edmite central para sumas (1 - 12)</li> </ul>"},{"location":"3_12_2_limite_central_muestras/","title":"Teorema del l\u00edmite central para muestras","text":""},{"location":"3_12_2_limite_central_muestras/#presentacion","title":"Presentaci\u00f3n","text":"<p>12 - Teorema del l\u00edmite central: Desigualdades de Chebyshev y Markov y ley de los grandes n\u00fameros</p>"},{"location":"3_12_2_limite_central_muestras/#secciones","title":"Secciones","text":"<ul> <li>Teorema del l\u00edmite central para muestras (13 - 18)</li> </ul>"},{"location":"3_12_3_desigualdad_chebyshev/","title":"Desigualdad de Chebyshev","text":""},{"location":"3_12_3_desigualdad_chebyshev/#presentacion","title":"Presentaci\u00f3n","text":"<p>12 - Teorema del l\u00edmite central: Desigualdades de Chebyshev y Markov y ley de los grandes n\u00fameros</p>"},{"location":"3_12_3_desigualdad_chebyshev/#secciones","title":"Secciones","text":"<ul> <li>Desigualdad de Chebyshev (19 - 29)</li> </ul>"},{"location":"3_12_4_desigualdad_markov/","title":"Desigualdad de Markov y ley de los n\u00fameros grandes","text":""},{"location":"3_12_4_desigualdad_markov/#presentacion","title":"Presentaci\u00f3n","text":"<p>12 - Teorema del l\u00edmite central: Desigualdades de Chebyshev y Markov y ley de los grandes n\u00fameros</p>"},{"location":"3_12_4_desigualdad_markov/#secciones","title":"Secciones","text":"<ul> <li>Desigualdad de Markov (21 - 30)</li> <li>Ley de los n\u00fameros grandes (31 - 34)</li> </ul>"},{"location":"3_9_1_variable_aleatoria_multiple/","title":"Funci\u00f3n de probabilidad acumulativa conjunta","text":""},{"location":"3_9_1_variable_aleatoria_multiple/#presentacion","title":"Presentaci\u00f3n","text":"<p>9 - Variables aleatorias m\u00faltiples</p>"},{"location":"3_9_1_variable_aleatoria_multiple/#secciones","title":"Secciones","text":"<ul> <li>Variables aleatorias m\u00faltiples (I-VI) (1 - 6)</li> <li>Funci\u00f3n acumulativa conjunta (7 - 10)</li> </ul>"},{"location":"3_9_2_propiedades_acumulativa/","title":"Funci\u00f3n de densidad probabil\u00edstica conjunta","text":""},{"location":"3_9_2_propiedades_acumulativa/#presentacion","title":"Presentaci\u00f3n","text":"<p>9 - Variables aleatorias m\u00faltiples</p>"},{"location":"3_9_2_propiedades_acumulativa/#secciones","title":"Secciones","text":"<ul> <li>Funci\u00f3n acumulativa conjunta (11 - 15)</li> <li>Funci\u00f3n de densidad probabil\u00edstica conjunta (16-20)</li> </ul>"},{"location":"3_9_3_propiedades_densidad_conjunta/","title":"Propiedades de la funci\u00f3n de densidad conjunta","text":""},{"location":"3_9_3_propiedades_densidad_conjunta/#presentacion","title":"Presentaci\u00f3n","text":"<p>9 - Variables aleatorias m\u00faltiples</p>"},{"location":"3_9_3_propiedades_densidad_conjunta/#secciones","title":"Secciones","text":"<ul> <li>Propiedades de la funci\u00f3n de densidad conjunta (21 - 27)</li> </ul>"},{"location":"3_9_4_independencia_estadistica/","title":"Independencia estad\u00edstica","text":""},{"location":"3_9_4_independencia_estadistica/#presentacion","title":"Presentaci\u00f3n","text":"<p>9 - Variables aleatorias m\u00faltiples</p>"},{"location":"3_9_4_independencia_estadistica/#secciones","title":"Secciones","text":"<ul> <li>Independencia estad\u00edstica (34 - 38)</li> </ul>"},{"location":"4_0_introduccion/","title":"Introducci\u00f3n","text":"<p>Los procesos aleatorios son los terceros \u201cobjetos aleatorios\u201d por analizar. Incorporan una segunda variable independiente, el tiempo, que los hace \u00fatiles en la descripci\u00f3n de fen\u00f3menos cambiantes o din\u00e1micos tales como las se\u00f1ales y los sistemas.</p>"},{"location":"4_13_1_concepto_estocastico/","title":"Definiciones y concepto de un proceso estoc\u00e1stico","text":""},{"location":"4_13_1_concepto_estocastico/#presentacion","title":"Presentaci\u00f3n","text":"<p>13 - Procesos aleatorios</p>"},{"location":"4_13_1_concepto_estocastico/#secciones","title":"Secciones","text":"<ul> <li>Definiciones  (1 - 7)</li> <li>Concepto de un proceso estoc\u00e1stico  (8 - 14)</li> </ul>"},{"location":"4_13_2_clasificacion/","title":"Clasificaci\u00f3n de procesos","text":""},{"location":"4_13_2_clasificacion/#presentacion","title":"Presentaci\u00f3n","text":"<p>13 - Procesos aleatorios</p>"},{"location":"4_13_2_clasificacion/#secciones","title":"Secciones","text":"<ul> <li>Clasificaci\u00f3n de procesos  (15 - 16)</li> <li>Procesos determin\u00edsticos y no determin\u00edsticos (17-21)</li> </ul>"},{"location":"4_13_3_funciones_distribucion/","title":"Funciones de distribuci\u00f3n e independencia estad\u00edstica","text":""},{"location":"4_13_3_funciones_distribucion/#presentacion","title":"Presentaci\u00f3n","text":"<p>13 - Procesos aleatorios</p>"},{"location":"4_13_3_funciones_distribucion/#secciones","title":"Secciones","text":"<ul> <li>Funciones de distribuci\u00f3n de un proceso aleatorio  (22 - 28)</li> <li>Independencia estad\u00edstica (29)</li> </ul>"},{"location":"4_13_4_estacionaridad/","title":"Estacionaridad","text":""},{"location":"4_13_4_estacionaridad/#presentacion","title":"Presentaci\u00f3n","text":"<p>13 - Procesos aleatorios</p>"},{"location":"4_13_4_estacionaridad/#secciones","title":"Secciones","text":"<ul> <li>Estacionaridad  (30 - 42)</li> </ul>"},{"location":"4_14_1_ergodicidad/","title":"Promedios en el tiempo y ergodicidad","text":"<p>Introducci\u00f3n</p> <p>La ergodicidad establece la igualdad entre el promedio estad\u00edstico y el promedio temporal de un proceso aleatorio.</p> <p>Es una nueva forma de estacionaridad que simplifica el an\u00e1lisis del proceso aleatorio.</p>"},{"location":"4_14_1_ergodicidad/#promedios-en-el-tiempo","title":"Promedios en el tiempo","text":"<p>El promedio temporal de una funci\u00f3n est\u00e1 definido con el nuevo operador</p> \\[\\begin{equation}   A \\left[~ f(t) ~\\right] = \\lim_{T \\rightarrow \\infty} \\frac{1}{2T} \\int_{-T}^{T} f(t)\\, \\mathrm{d}t \\end{equation}\\] <p>donde \\(A\\) se utiliza para denotar promedio temporal de una manera an\u00e1loga al operador \\(E\\) para el promedio estad\u00edstico. \\(f(t)\\) es una funci\u00f3n del tiempo cualquiera.</p> <p> </p> <p>El valor \\(\\overline{x} = A[x(t)]\\) representa el promedio temporal de una funci\u00f3n muestra. La funci\u00f3n de autocorrelaci\u00f3n temporal es denotada por \\(\\mathcal{R}_{XX}(\\tau) = A[x(t)x(t+\\tau)]\\). Estas funciones est\u00e1n definidas por</p> \\[\\begin{equation} \\begin{aligned}   \\overline{x} &amp; = A[x(t)] \\\\             &amp; = \\lim_{T \\rightarrow \\infty} \\frac{1}{2T}\\int_{-T}^{T}x(t)\\, \\mathrm{d}t \\end{aligned} \\end{equation}\\] \\[\\begin{equation} \\begin{aligned}   \\mathcal{R}_{XX}(\\tau) &amp; = A[x(t)x(t+\\tau)] \\\\             &amp; = \\lim_{T \\rightarrow \\infty} \\frac{1}{2T} \\int_{-T}^{T}x(t)x(t+\\tau)\\, \\mathrm{d}t  \\end{aligned} \\end{equation}\\] <p>Para cualesquiera funci\u00f3n muestra \\(x(t)\\) del proceso \\(X(t)\\), estas dos \u00faltimas integrales simplemente producen dos n\u00fameros (para un valor fijo de \\(\\tau\\)).   </p> <p> Figura: Ejemplo de un proceso aleatorio llamado \"caminata aleatoria\" (random walk).  </p> <p>Cuando se consideran todas las funciones muestra, \\(\\overline{x}\\) y \\(\\mathcal{R}_{XX}(\\tau)\\) son realmente variables aleatorias. Tomando el valor esperado a ambos lados de las definiciones, suponiendo que la operaci\u00f3n matem\u00e1tica de la esperanza puede llevarse al interior de la integral y suponiendo que \\(X(t)\\) es un proceso estacionario, </p> \\[\\begin{equation*}     E[\\overline{x}] = \\overline{X} \\qquad\\qquad\\qquad     E[\\mathcal{R}_{XX}(\\tau)] = R_{XX}(\\tau)  \\end{equation*}\\]"},{"location":"4_14_1_ergodicidad/#procesos-ergodicos","title":"Procesos erg\u00f3dicos","text":"<p>Procesos erg\u00f3dicos</p> <p>Si se supone que \\(\\overline{x}\\) y \\(\\mathcal{R}_{XX}(\\tau)\\) tienen varianzas nulas, es decir, que son constantes, se escribe entonces, </p> \\[\\begin{equation*} \\begin{aligned} E[\\overline{x}] &amp; = \\overline{x} = \\overline{X} \\\\ E[\\mathcal{R}_{XX}(\\tau)] &amp; = \\mathcal{R}_{XX}(\\tau) = R_{XX}(\\tau) \\end{aligned} \\end{equation*}\\] <p>Los promedios temporales \\(\\overline{x}\\) y \\(\\mathcal{R}_{XX}(\\tau)\\) igualan a los promedios estad\u00edsticos. </p> <p>Los procesos para los que los promedios temporales igualan a los estad\u00edsticos se denominan erg\u00f3dicos. </p> <p> Figura: Ejemplo de un proceso aleatorio llamado \"ruido blanco\" (white noise).</p> <p>Ergodicidad es una forma muy restrictiva de estacionaridad y puede ser dif\u00edcil probar que constituye una suposici\u00f3n razonable para cualquier situaci\u00f3n f\u00edsica. Sin embargo, se asumir\u00e1 que un proceso es erg\u00f3dico a veces para simplificar problemas. </p> <p>Ergodicidad conjunta Dos procesos aleatorios son llamados conjuntamente erg\u00f3dicos si son individualmente erg\u00f3dicos y tambi\u00e9n tienen una funci\u00f3n de correlaci\u00f3n cruzada temporal que iguala la funci\u00f3n de correlaci\u00f3n cruzada estad\u00edstica: </p> \\[\\begin{equation}   \\mathcal{R}_{XY}(\\tau) = \\lim_{T \\rightarrow \\infty}\\frac{1}{2T}\\int_{-T}^{T}x(t)y(t+\\tau)\\, \\mathrm{d}{t} = R_{XY}(\\tau) \\end{equation}\\]"},{"location":"4_14_2_correlacion_covarianza/","title":"Funciones de correlaci\u00f3n y covarianza","text":""},{"location":"4_14_2_correlacion_covarianza/#presentacion","title":"Presentaci\u00f3n","text":"<p>14 - Ergodicidad y funciones de correlaci\u00f3n</p>"},{"location":"4_14_2_correlacion_covarianza/#secciones","title":"Secciones","text":"<ul> <li>Funciones de correlaci\u00f3n y covarianza (8 - 15)</li> </ul>"},{"location":"4_14_3_funciones_covarianza/","title":"Funciones de covarianza","text":""},{"location":"4_14_3_funciones_covarianza/#presentacion","title":"Presentaci\u00f3n","text":"<p>14 - Ergodicidad y funciones de correlaci\u00f3n</p>"},{"location":"4_14_3_funciones_covarianza/#secciones","title":"Secciones","text":"<ul> <li>Funciones de covarianza (16 - 19)</li> </ul>"},{"location":"4_15_1_espectro_densidad/","title":"Espectro de densidad de potencia y sus propiedades","text":""},{"location":"4_15_1_espectro_densidad/#presentacion","title":"Presentaci\u00f3n","text":"<p>15 - Caracter\u00edsticas espectrales de los procesos aleatorios</p>"},{"location":"4_15_1_espectro_densidad/#secciones","title":"Secciones","text":"<ul> <li>Espectro de densidad de potencia y sus propiedades (1 - 13)</li> </ul>"},{"location":"4_15_2_propiedades/","title":"Propiedades de la densidad espectral y definiciones de ruido","text":""},{"location":"4_15_2_propiedades/#presentacion","title":"Presentaci\u00f3n","text":"<p>15 - Caracter\u00edsticas espectrales de los procesos aleatorios</p>"},{"location":"4_15_2_propiedades/#secciones","title":"Secciones","text":"<ul> <li>Propiedades de la densidad espectral de potencia (14 - 20)</li> <li>Algunas definiciones de ruido (21-29)</li> </ul>"},{"location":"4_16_1_respuesta/","title":"Respuesta del sistema y valor cuadr\u00e1tico medio","text":""},{"location":"4_16_1_respuesta/#presentacion","title":"Presentaci\u00f3n","text":"<p>16 - Respuesta de sistemas lineales a una se\u00f1al aleatoria</p>"},{"location":"4_16_1_respuesta/#secciones","title":"Secciones","text":"<ul> <li>Respuesta del sistema (1 - 2)</li> <li>Valor cuadr\u00e1tico medio (2 - 6)</li> </ul>"},{"location":"4_16_2_autocorrelacion_caracteristicas/","title":"Autocorrelaciones y caracter\u00edsticas espectrales de la respuesta del sistema","text":""},{"location":"4_16_2_autocorrelacion_caracteristicas/#presentacion","title":"Presentaci\u00f3n","text":"<p>16 - Respuesta de sistemas lineales a una se\u00f1al aleatoria</p>"},{"location":"4_16_2_autocorrelacion_caracteristicas/#secciones","title":"Secciones","text":"<ul> <li>Autocorrelaciones de entrada y salida y correlaciones cruzadas (7 - 14)</li> <li>Caracter\u00edsticas espectrales de la respuesta del sistema (15 - 20)</li> </ul>"},{"location":"5_0_introduccion/","title":"Introducci\u00f3n","text":"<p>Los procesos aleatorios son los terceros \u201cobjetos aleatorios\u201d por analizar. Incorporan una segunda variable independiente, el tiempo, que los hace \u00fatiles en la descripci\u00f3n de fen\u00f3menos cambiantes o din\u00e1micos tales como las se\u00f1ales y los sistemas.</p>"},{"location":"5_17_1_proceso_funciones_poisson/","title":"Proceso aleatorio de Poisson y sus funciones de densidad","text":""},{"location":"5_17_1_proceso_funciones_poisson/#presentacion","title":"Presentaci\u00f3n","text":"<p>17 - Proceso contador de Poisson</p>"},{"location":"5_17_1_proceso_funciones_poisson/#secciones","title":"Secciones","text":"<ul> <li>Proceso aleatorio de Poisson (1 - 6)</li> <li>Funciones de densidad del proceso de Poisson (7 - 9)</li> </ul>"},{"location":"5_17_2_momento_densidad/","title":"Momentos de la distribuci\u00f3n de Poisson y probabilidad conjunta y condicional","text":""},{"location":"5_17_2_momento_densidad/#presentacion","title":"Presentaci\u00f3n","text":"<p>17 - Proceso contador de Poisson</p>"},{"location":"5_17_2_momento_densidad/#secciones","title":"Secciones","text":"<ul> <li>Momentos de la distribuci\u00f3n de Poisson(10 - 11)</li> <li>Probabilidad conjunta y condicional del proceso de Poisson (12 - 22)</li> </ul>"},{"location":"5_18_1_definicion_markov/","title":"Ejemplo de cadenas de Markov y deducci\u00f3n","text":""},{"location":"5_18_1_definicion_markov/#presentacion","title":"Presentaci\u00f3n","text":"<p>18 - Cadenas de Markov en tiempo cont\u00ednuo</p>"},{"location":"5_18_1_definicion_markov/#secciones","title":"Secciones","text":"<ul> <li>Ejemplo de cadenas de Markov (1 - 4)</li> <li>Deducci\u00f3n a partir de la distribuci\u00f3n exponencial (5 - 10)</li> </ul>"},{"location":"5_18_2_nacimiento_muerte/","title":"Proceso de nacimiento y muerte en tiempo continuo","text":""},{"location":"5_18_2_nacimiento_muerte/#presentacion","title":"Presentaci\u00f3n","text":"<p>18 - Cadenas de Markov en tiempo cont\u00ednuo</p>"},{"location":"5_18_2_nacimiento_muerte/#secciones","title":"Secciones","text":"<ul> <li>Proceso de nacimiento y muerte en tiempo cont\u00ednuo (11 - 18)</li> </ul>"},{"location":"5_18_3_teoria_de_colas/","title":"Teor\u00eda de colas","text":""},{"location":"5_18_3_teoria_de_colas/#presentacion","title":"Presentaci\u00f3n","text":"<p>18 - Cadenas de Markov en tiempo cont\u00ednuo</p>"},{"location":"5_18_3_teoria_de_colas/#secciones","title":"Secciones","text":"<ul> <li>Teor\u00eda de colas (19-37)</li> </ul>"},{"location":"5_19_1_vector_estable/","title":"Vector de probabilidad de estado estable","text":""},{"location":"5_19_1_vector_estable/#presentacion","title":"Presentaci\u00f3n","text":"<p>19 - Vector de probabilidad de estado estable</p>"},{"location":"5_19_1_vector_estable/#secciones","title":"Secciones","text":"<ul> <li>Vector de probabilidad de estado estable (1 - 13)</li> <li>Estado estable para el tel\u00e9fono con bot\u00f3n de retenci\u00f3n (14 - 17)</li> </ul>"},{"location":"5_19_2_cola_servidores/","title":"Cola de un solo servidor y de un n\u00famero infinito","text":""},{"location":"5_19_2_cola_servidores/#presentacion","title":"Presentaci\u00f3n","text":"<p>19 - Vector de probabilidad de estado estable</p>"},{"location":"5_19_2_cola_servidores/#secciones","title":"Secciones","text":"<ul> <li>Cola de un solo servidor (18 - 23)</li> <li>Cola de un infinito n\u00famero de servidores (24 - 29)</li> </ul>"},{"location":"5_19_3_sistema_colas_mms/","title":"Sistema de colas M/M/s","text":""},{"location":"5_19_3_sistema_colas_mms/#presentacion","title":"Presentaci\u00f3n","text":"<p>19 - Vector de probabilidad de estado estable</p>"},{"location":"5_19_3_sistema_colas_mms/#secciones","title":"Secciones","text":"<ul> <li>Sistema de colas M/M/s (30 - 41)</li> </ul>"},{"location":"5_20_1_markov_discreto/","title":"Cadenas de Markov en tiempo discreto","text":""},{"location":"5_20_1_markov_discreto/#presentacion","title":"Presentaci\u00f3n","text":"<p>20 - Cadenas de Markov en tiempo discreto</p>"},{"location":"5_20_1_markov_discreto/#secciones","title":"Secciones","text":"<ul> <li>Cadenas de Markov en tiempo discreto (1 - 18)</li> </ul>"},{"location":"5_20_2_markov_discreto/","title":"La matriz de transici\u00f3n de orden t","text":""},{"location":"5_20_2_markov_discreto/#presentacion","title":"Presentaci\u00f3n","text":"<p>20 - Cadenas de Markov en tiempo discreto</p>"},{"location":"5_20_2_markov_discreto/#secciones","title":"Secciones","text":"<ul> <li>La matriz de transici\u00f3n de orden t (19 - 24)</li> </ul>"},{"location":"5_20_3_markov_discreto/","title":"Ecuaciones de Chapman - Kolgomorov y Vector de probabilidad pt","text":""},{"location":"5_20_3_markov_discreto/#presentacion","title":"Presentaci\u00f3n","text":"<p>21 - Cadenas de Markov en tiempo discreto</p>"},{"location":"5_20_3_markov_discreto/#secciones","title":"Secciones","text":"<ul> <li>Ecuaciones de Chapman - Kolgomorov (25 - 28)</li> <li>Vector de probabilidad pt (29 - 37)</li> </ul>"},{"location":"5_21_1_markov_estado_estable/","title":"El vector de probabilidad en estado estable","text":"<p>Introducci\u00f3n</p> <p>Cuando existe una cantidad definida de estados, es posible modelar las transiciones entre todos estos estados.</p> <p>Luego de suficientes transiciones, y al alcanzar un \"r\u00e9gimen permanente\", cada estado tiene una probabilidad definida.</p> <p>Como en el caso anterior (no discreto), sup\u00f3ngase que hay un n\u00famero grande \\(N\\) de part\u00edculas, cada una que salta de estado a estado entre los estados de \\(S\\) guiados por la matriz de transici\u00f3n \\(\\Pi\\) de probabilidades de saltos.</p> <ul> <li>Si todas las \\(N\\) part\u00edculas empiezan en el estado 0 en el tiempo \\(t = 0\\), entonces despu\u00e9s de un salto algunas permanecer\u00e1n en el estado 0 (si \\(\\Pi_{0,0} &gt; 0\\)) y otras saltar\u00e1n a otros estados. Se puede esperar \\(N\\Pi_{0,j}\\) part\u00edculas en el estado \\(j\\) despu\u00e9s de un salto.</li> <li>Por otro lado, sup\u00f3ngase que se distribuyen las \\(N\\) part\u00edculas de modo que \\(N_{j}\\) empiezan en el estado \\(j\\) en el tiempo 0 para \\(j = 0, 1, 2, \\ldots, N\\).</li> <li>Dado que \\(N_{j}\\Pi_{j,i}\\) de aquellas part\u00edculas que empiezan en \\(j\\) puede esperarse que salten al estado \\(i\\), el n\u00famero total de part\u00edculas que puede esperarse que est\u00e9n en el estado \\(i\\) despu\u00e9s de un salto es:</li> </ul> \\[\\begin{equation} \\sum_{j=1}^{N} N_{j} \\Pi_{j,i} \\end{equation}\\] <ul> <li>Pudiera suceder que este n\u00famero es el mismo n\u00famero \\(N_{i}\\) de part\u00edculas que empezaron en el estado \\(i\\) en el tiempo 0.</li> </ul> <p>Cada una de las part\u00edculas podr\u00eda cambiar estados, pero el n\u00famero completo en el estado \\(i\\) permanecer\u00e1 constante. Si esto fuera cierto para cada estado \\(i \\in S\\), el sistema entero de \\(N\\) part\u00edculas estar\u00eda en estado estable: por cada part\u00edcula que deja un estado, una la reemplazar\u00eda proveniente de otro estado.</p> <p>En vez del n\u00famero \\(N_{i}\\) absoluto de part\u00edculas en estado \\(i\\), </p> \\[\\begin{equation}   N_{i} = \\sum_{j=0}^{N} N_{j}\\Pi_{j,i} \\end{equation}\\] <p>reestabl\u00e9zcase la ecuaci\u00f3n en t\u00e9rminos del n\u00famero relativo \\(N_{i}/N\\) de part\u00edculas en estado \\(i\\).</p> <p>Por tanto, es la probabilidad de que cualquier part\u00edcula ocupe el estado \\(i\\)</p> \\[\\begin{equation}   N_{i}/N = \\sum_{j=0}^{N} \\left( N_{j}/N \\right) \\Pi_{j,i} \\end{equation}\\] <p>Si este fuera el caso, el sistema entero de \\(N\\) part\u00edculas estar\u00eda en el estado estable. </p> <p>Un vector de probabilidad \\(\\phi\\) representa el estado estable si</p> \\[\\begin{equation}   \\phi_{i} = \\sum_{j=0}^{N} \\phi_{j}\\Pi_{j,i} \\end{equation}\\] <p>o sea, si \\(\\phi^{1} = \\phi \\cdot \\Pi = \\phi\\). De esta forma, la probabilidad de que una part\u00edcula est\u00e9 en el estado \\(i\\) es la misma en el tiempo 1 como en el tiempo 0. </p> <p>N\u00f3tese que si \\(\\phi\\) tiene esta propiedad de reproducirse a s\u00ed mismo despu\u00e9s de un salto, esto se cumplir\u00e1 para todos los tiempos \\(t\\): </p> \\[\\begin{equation}   \\phi^{1} = \\phi \\Pi = \\phi \\end{equation}\\] <p>lo que implica</p> \\[\\begin{equation} \\begin{aligned}   \\phi^{2} &amp;= \\phi^{1} \\Pi = \\phi \\Pi = \\phi \\\\   \\phi^{3} &amp;= \\phi^{2} \\Pi = \\phi \\Pi = \\phi \\end{aligned}  \\end{equation}\\] <p>y, en general, </p> \\[\\begin{equation}  \\begin{aligned}   \\phi^{t}  &amp; = \\phi^{t-1}\\Pi \\\\             &amp; = \\phi \\Pi \\\\             &amp; = \\phi \\end{aligned}  \\end{equation}\\] <p>Vector de probabilidad de estado estable</p> <p>Cualquier vector de probabilidad con la propiedad \\(\\phi = \\phi \\Pi\\) es denominado un vector de probabilidad de estado estable. Si la part\u00edcula empieza en el estado \\(i\\) con probabilidad \\(\\phi_{i}\\) por cada estado \\(i\\), entonces en todo tiempo \\(t\\), estar\u00e1 en el estado \\(i\\) con probabilidad \\(\\phi_{i}\\).</p>"},{"location":"5_21_1_markov_estado_estable/#procedimiento-para-hallar-el-vector-de-probabilidad-de-estado-estable","title":"Procedimiento para hallar el vector de probabilidad de estado estable","text":"<p>Consta de dos pasos:</p> <ul> <li>Establecer y resolver estas ecuaciones:</li> </ul> \\[\\begin{equation} \\phi_{j} = \\sum_{i=0}^{N} \\phi_{i} \\Pi_{i,j} \\end{equation}\\] <p>para \\(j = 0, 1, 2, \\ldots, N\\) o alternativamente, en notaci\u00f3n matricial, \\(\\phi = \\phi \\Pi\\).</p> <ul> <li>Normalizar por medio de la ecuaci\u00f3n:</li> </ul> \\[\\begin{equation} \\sum_{i=0}^{N} \\phi_{i} = 1 \\end{equation}\\] <p>Notas:</p> <ul> <li>El paso 1 anterior involucra la soluci\u00f3n de \\(N+1\\) ecuaciones para \\(N+1\\) inc\u00f3gnitas \\(\\phi_{0}, \\phi_{1}, \\phi_{2}, \\ldots, \\phi_{N}\\). Siempre habr\u00e1 redundancia: una de las ecuaciones ser\u00e1 una combinaci\u00f3n lineal de las otras.</li> <li>La ecuaci\u00f3n del paso 2 es realmente la \\((N+1)\\)-\u00e9sima.</li> <li>En otras palabras: el primer paso, aunque define un sistema de \\(N+1\\) ecuaciones, solamente \\(N\\) de ellas son linealmente independientes, por lo que se necesita del paso 2 para proveer la \\((N+1)\\)-\u00e9sima ecuaci\u00f3n para poder encontrar las \\(N+1\\) inc\u00f3gnitas, que definir\u00e1n los componentes del vector de probabilidad de estado estable.</li> </ul> <p> EJEMPLO</p> <p>Vector de probabilidad de estado estable con dos estados</p> <p>Encuentre el vector de probabilidad de estado estable de la cadena de Markov mostrada en la figura siguiente. </p> <p></p> <p>Aplica que: </p> \\[\\begin{equation}  \\begin{aligned} \\phi &amp; = \\phi \\Pi \\\\ (\\phi_{0}, \\phi_{1}) &amp; = (\\phi_{0}, \\phi_{1}) \\Pi \\\\ \\, &amp; = (\\phi_{0}, \\phi_{1}) \\left[ \\begin{array}{cc} \\frac{1}{2} &amp; \\frac{1}{2} \\\\ 1   &amp; 0  \\end{array} \\right] \\\\ \\phi_{0} &amp; = \\frac{1}{2}\\phi_{0} + \\phi_{1} \\\\ \\phi_{1} &amp; = \\frac{1}{2}\\phi_{0}  \\end{aligned} \\end{equation}\\] <p>Las dos \u00faltimas ecuaciones son realmente la misma: \\(\\phi_{1} = \\frac{1}{2}\\phi_{0}\\). A continuaci\u00f3n se usa la condici\u00f3n de normalizaci\u00f3n: </p> \\[\\begin{equation} 1 = \\phi_{0} + \\phi_{1} = \\phi_{0} + \\frac{1}{2}\\phi_{0} = \\frac{3}{2}\\phi_{0} \\end{equation}\\] <p>De esta forma se concluye que \\(\\phi_{0} = 2/3, \\phi_{1} = 1/3\\). Por consiguiente, dos terceras partes del tiempo, la part\u00edcula se encontrar\u00e1 en el estado 0 y una tercera parte del tiempo se encontrar\u00e1 en el estado 1.</p> <p> EJEMPLO</p> <p>Vector de probabilidad de estado estable con tres estados</p> <p>Considere la cadena de Markov de la siguiente figura. Encuentre el vector de probabilidad de estado estable \\(\\phi\\).</p> <p> </p> <p>Se construye primero la matriz de transici\u00f3n \\(\\Pi\\): </p> \\[\\begin{equation} \\Pi = \\left[ \\begin{array}{ccc} 0 &amp; \\frac{1}{2} &amp; \\frac{1}{2} \\\\ 0 &amp; \\frac{2}{3} &amp; \\frac{1}{3} \\\\ \\frac{1}{3} &amp; \\frac{2}{3} &amp; 0 \\end{array} \\right] \\end{equation}\\] \\[\\begin{equation}  \\begin{aligned} (\\phi_{0}, \\phi_{1}, \\phi_{2}) &amp; = (\\phi_{0}, \\phi_{1}, \\phi_{2}) \\left[ \\begin{array}{ccc} 0 &amp; \\frac{1}{2} &amp; \\frac{1}{2} \\\\ 0 &amp; \\frac{2}{3} &amp; \\frac{1}{3} \\\\ \\frac{1}{3} &amp; \\frac{2}{3} &amp; 0 \\end{array} \\right] \\\\ \\phi_{0} &amp; = \\frac{1}{3}\\phi_{2} \\\\ \\phi_{1} &amp; = \\frac{1}{2}\\phi_{0} + \\frac{2}{3}\\phi_{1} + \\frac{2}{3}\\phi_{2} \\\\ \\phi_{2} &amp; = \\frac{1}{2}\\phi_{0} + \\frac{1}{3}\\phi_{1} \\end{aligned}  \\end{equation}\\] <p>lo cual genera, </p> \\[\\begin{equation}  \\begin{aligned} -3\\phi_{0} + \\phi_{2} &amp; = 0 \\\\ 3\\phi_{0} - 2\\phi_{1} + 4\\phi_{2} &amp; = 0 \\\\ 3\\phi_{0} + 2\\phi_{1} - 6\\phi_{2} &amp; = 0 \\end{aligned}  \\end{equation}\\] <p>Si se suma la segunda y la tercera ecuaciones, resulta en \\(6\\phi_{0} - 2\\phi_{2} = 0\\), que es esencialmente la misma primera ecuaci\u00f3n, de donde se puede decir que la tercera ecuaci\u00f3n es redundante. De las primeras dos ecuaciones se tiene que, </p> \\[\\begin{equation}  \\begin{aligned} \\phi_{2} &amp; = 3\\phi_{0} \\\\  \\phi_{1} &amp; = \\frac{15}{2}\\phi_{0}  \\end{aligned}  \\end{equation}\\] <p>Toca ahora usar la condici\u00f3n de normalizaci\u00f3n: </p> \\[\\begin{equation} 1 = \\phi_{0} + \\phi_{1} + \\phi_{2} = \\left( 1 + \\frac{15}{2} + 3\\right) \\phi_{0} = \\frac{23}{2}\\phi_{0} \\end{equation}\\] <p>Por consiguiente, \\(\\phi_{0} = \\frac{2}{23}, \\phi_{1} = \\frac{15}{23}, \\phi_{2} = \\frac{6}{23}\\).</p>"},{"location":"5_21_2_markov_estado_estable/","title":"Proceso c\u00edclico","text":""},{"location":"5_21_2_markov_estado_estable/#presentacion","title":"Presentaci\u00f3n","text":"<p>21 - Cadenas de Markov en tiempo discreto y vector de estado estable</p>"},{"location":"5_21_2_markov_estado_estable/#secciones","title":"Secciones","text":"<ul> <li>Proceso c\u00edclico (16 - 25)</li> </ul>"},{"location":"transcripcion/","title":"Sobre la transcripci\u00f3n","text":"<p>El objetivo de esta tarea es hacer una transcripci\u00f3n de los contenidos del curso Modelos Probabil\u00edsticos de Se\u00f1ales y Sistemas de las presentaciones hechas en LaTeX (Overleaf) para documentos en formato Markdown.</p> <p>Esto tiene los siguientes objetivos:</p> <ul> <li>Explotar la versatilidad del formato Markdown para incluir nuevas formas de contenido multimedia.</li> <li>Crear una versi\u00f3n de p\u00e1gina web del contenido, visible f\u00e1cilmente desde celulares u otros medios, con MkDocs (esta misma p\u00e1gina es un ejemplo).</li> <li>Crear presentaciones interactivas con Slidev. Las presentaciones PDF actuales son, por su naturaleza, est\u00e1ticas.</li> <li>Crear un documento en LaTeX con los contenidos del curso pero en formato de libro de texto, utilizando los mismos archivos Markdown (<code>.md</code>) como fuente.</li> </ul> <p>Transcripci\u00f3n de referencia</p> <p>El documento <code>5_21_1_markov_estado_estable.md</code> tiene la transcripci\u00f3n de la presentaci\u00f3n 21 \"El vector de probabilidad en estado estable\".</p> <p>Por favor ver esa p\u00e1gina.</p>"},{"location":"transcripcion/#latex-vs-markdown","title":"LaTeX vs. Markdown","text":"<p>Tanto LaTeX como Markdown son lenguajes de descripci\u00f3n de formato. Markdown, de hecho, es una versi\u00f3n de sintaxis simplificada de HTML.</p> <p>Estas son algunas equivalencias:</p> Formato LaTeX Markdown HTML Cursiva <code>\\textit{hola}</code> <code>*hola*</code> <code>&lt;emph&gt;hola&lt;/emph&gt;</code> Negrita <code>\\textbf{hola}</code> <code>**hola**</code> <code>&lt;strong&gt;hola&lt;/strong&gt;</code> Secci\u00f3n <code>\\section{Hola}</code> <code># Hola</code> <code>&lt;h1&gt;Hola&lt;/h1&gt;</code> Subsecci\u00f3n <code>\\subsection{Hola}</code> <code>## Hola</code> <code>&lt;h2&gt;Hola&lt;/h2&gt;</code>"},{"location":"transcripcion/#elementos-de-la-transcripcion","title":"Elementos de la transcripci\u00f3n","text":"<p>La mayor\u00eda de las presentaciones tiene algunos o todos de los siguientes elementos.</p> <ul> <li> <p> Textos</p> <p>Textos con formato como negrita, cursiva, t\u00edtulos, listas y otros.</p> <p> Ver detalles</p> </li> <li> <p> F\u00f3rmulas</p> <p>Variables y ecuaciones en l\u00ednea y ecuaciones en bloque.</p> <p> Ver detalles</p> </li> <li> <p> Gr\u00e1ficos</p> <p>Gr\u00e1ficos generados con PGF/TikZ y exportados como SVG.</p> <p> Ver detalles</p> </li> <li> <p> Notas especiales</p> <p>Secciones con formato especial para dar \u00e9nfasis a afirmaciones o resultados.</p> <p> Ver detalles</p> </li> </ul>"},{"location":"transcripcion/#textos","title":"Textos","text":"<p>Los textos, listas y otros similares siguen la sintaxis b\u00e1sica de Markdown.</p> <p>Observaciones</p> <ul> <li>No todos los t\u00edtulos de diapositivas (<code>frame</code>) deben incluirse, solamente los que sean significativos y no redundantes.</li> <li>Respetar el uso de \u00e9nfasis con negritas y/o cursivas.</li> <li>Cuando hay \u00e9nfasis con cajas de colores, ver secci\u00f3n notas especiales.</li> <li>Es posible utilizar las herramientas de Material for MkDocs, pero por el momento es mejor mantener el texto con formato b\u00e1sico.</li> <li>Deben ignorar (y eliminar) las siguientes instrucciones de LaTeX:<ul> <li><code>\\begin{frame} ... \\end{frame}</code> (excepto los t\u00edtulos, cuando corresponde)</li> <li><code>\\framebreak</code></li> <li><code>\\noindent</code></li> </ul> </li> </ul>"},{"location":"transcripcion/#formulas","title":"F\u00f3rmulas","text":"<p>Todas las f\u00f3rmulas son escritas con notaci\u00f3n de LaTeX. Esta documentaci\u00f3n utiliza MathJax para renderizar las f\u00f3rmulas en el navegador. La notaci\u00f3n es id\u00e9ntica:</p> LaTeXMarkdown <pre><code>\\begin{equation}\nx_{1,2} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n\\end{equation}\n</code></pre> <pre><code>\\begin{equation}\nx_{1,2} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n\\end{equation}\n</code></pre> <p>que genera:</p> \\[\\begin{equation} x_{1,2} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\end{equation}\\] <p>Hay variables y expresiones como \\(x_{1,2}\\) o \\(b^2\\), y ecuaciones en l\u00ednea, como \\(a \\neq 0\\), y es muy importante usar la notaci\u00f3n matem\u00e1tica dentro del texto. Por ejemplo, la variable aleatoria es \\(X\\) y no X.</p> <p>Observaciones</p> <ul> <li>Eliminar el <code>*</code> en <code>\\begin{equation*}</code>, que indica ecuaci\u00f3n no numerada (porque no aplica aqu\u00ed).</li> <li>Si existiese el delimitador <code>\\[ ... \\]</code> para ecuaciones en bloque, sustituirlo por <code>\\begin{equation} ... \\end{equation}</code>. </li> <li>En las integrales, sustituir el comando especial <code>\\di{x}</code> por <code>~\\mathrm{d} x</code>, que describe el operador \\(\\mathrm{d} x\\) en una integral, como en \\(\\int_{-\\infty}^{\\infty} f_{X}(x) ~\\mathrm{d}x = 1\\).</li> <li>Al finalizar la transcripci\u00f3n, revisar por favor minuciosamente la correspondencia de las ecuaciones.</li> </ul>"},{"location":"transcripcion/#graficos","title":"Gr\u00e1ficos","text":"<p>La gran mayor\u00eda de los gr\u00e1ficos en las presentaciones son hechos con PGF/TikZ, un poderoso paquete de dise\u00f1o de im\u00e1genes.</p> <p>El directorio <code>images/</code> tiene la versi\u00f3n SVG de todas las im\u00e1genes de todas las presentaciones y deben usarlas en su lugar respectivo. El formato es:</p> <pre><code>![Descripci\u00f3n de la imagen](images/prueba.svg)\n</code></pre> <p>que muestra:</p> <p></p> <p>Observaciones</p> <ul> <li>Las referencias dentro del texto a una figura con <code>\\ref{label}</code> (si las hubiese) deben ser sustituidas por una descripci\u00f3n literal, como \"la figura anterior\" o \"la figura siguiente\", por ejemplo.</li> <li>Es necesaria una descripci\u00f3n adecuada de la imagen dentro de los par\u00e9ntesis cuadrados. Por ejemplo: <code>![Funci\u00f3n de densidad de probabilidad de la distribuci\u00f3n normal](densidad_normal.svg)</code>. Esta descripci\u00f3n es utilizada para accesibilidad de personas con discapacidad visual y para buscadores web.</li> </ul>"},{"location":"transcripcion/#notas-especiales","title":"Notas especiales","text":"<p>En las presentaciones hay varias \"cajas de colores\" para resaltar ciertas informaciones y definiciones. Es posible transcribirlas en admoniciones de Markdown, cuyo formato es:</p> <pre><code>!!! note \"Nota\"\n\n    Hola, soy una nota\n</code></pre> <p>que se convierte en:</p> <p>Nota</p> <p>Hola, soy una nota</p> <p>En particular, ser\u00e1n transcritos como notas:</p> <ul> <li>Introducci\u00f3n de una presentaci\u00f3n</li> <li>Definiciones</li> <li>Resultados con \u00e9nfasis</li> <li>Ejemplos o ejercicios</li> </ul>"},{"location":"transcripcion/#introduccion-de-una-presentacion","title":"Introducci\u00f3n de una presentaci\u00f3n","text":"<p>Al inicio de cada presentaci\u00f3n hay una breve rese\u00f1a del tema. Esta misma rese\u00f1a estar\u00e1 al inicio de la p\u00e1gina transcrita en Markdown, como una admonici\u00f3n.</p> LaTeXMarkdown <pre><code>\\begin{frame}[plain,noframenumbering]\n\\LARGE\nCuando existe una cantidad definida de estados, es posible modelar las transiciones entre todos estos estados.\n\nLuego de suficientes transiciones, y al alcanzar un ``r\u00e9gimen permanente'', cada estado tiene una probabilidad definida.\n\\end{frame}\n</code></pre> <pre><code>!!! abstract\n    Cuando existe una cantidad definida de estados, es posible modelar las transiciones entre todos estos estados.\n\n    Luego de suficientes transiciones, y al alcanzar un \"r\u00e9gimen permanente\", cada estado tiene una probabilidad definida.\n</code></pre> <p>y resulta en:</p> <p>Introducci\u00f3n</p> <p>Cuando existe una cantidad definida de estados, es posible modelar las transiciones entre todos estos estados.</p> <p>Luego de suficientes transiciones, y al alcanzar un \"r\u00e9gimen permanente\", cada estado tiene una probabilidad definida.</p>"},{"location":"transcripcion/#definiciones","title":"Definiciones","text":"<p>Las presentaciones tiene definiciones que est\u00e1n dentro de un entorno llamado <code>bloque</code>, que ser\u00e1 transcrito en una admonici\u00f3n tipo <code>tip</code>. Notar que hay un t\u00edtulo de descripci\u00f3n.</p> LaTeXMarkdown <pre><code>\\begin{bloque}{Definici\u00f3n de una variable aleatoria}\nPara un espacio de eventos $S$, una \\textbf{variable aleatoria} es cualquier regla que asocia cada resultado elemental de $S$ con \\textbf{un n\u00famero}. Es decir, es una \\textbf{funci\u00f3n} cuyo dominio es el espacio (quiz\u00e1 abstracto) de eventos o muestras, y cuyo rango es alg\u00fan subconjunto de los n\u00fameros reales.\n\\end{bloque}\n</code></pre> <pre><code>!!! tip \"Definici\u00f3n de una variable aleatoria\"\n    Para un espacio de eventos $S$, una **variable aleatoria** es cualquier regla que asocia cada resultado elemental de $S$ con **un n\u00famero**. Es decir, es una **funci\u00f3n** cuyo dominio es el espacio (quiz\u00e1 abstracto) de eventos o muestras, y cuyo rango es alg\u00fan subconjunto de los n\u00fameros reales.\n</code></pre> <p>y resulta en:</p> <p>Definici\u00f3n de una variable aleatoria</p> <p>Para un espacio de eventos \\(S\\), una variable aleatoria es cualquier regla que asocia cada resultado elemental de \\(S\\) con un n\u00famero. Es decir, es una funci\u00f3n cuyo dominio es el espacio (quiz\u00e1 abstracto) de eventos o muestras, y cuyo rango es alg\u00fan subconjunto de los n\u00fameros reales.</p>"},{"location":"transcripcion/#resultados-con-enfasis","title":"Resultados con \u00e9nfasis","text":"<p>Algunos resultados o frases tienen un \u00e9nfasis (caja verde, sin t\u00edtulo) con un entorno llamado <code>\u00e9nfasis</code>.</p> LaTeXMarkdown <pre><code>\\begin{enfasis}\nLa m\u00e1s famosa de las funciones de probabilidad es la \\textit{normal} (o \\emph{gaussiana}), que describe la noci\u00f3n de que las probabilidades de ocurrencia de un evento est\u00e1n concentradas de forma sim\u00e9trica alrededor de un valor central.\n\\end{enfasis}\n</code></pre> <pre><code>!!! note \"\"\n    La m\u00e1s famosa de las funciones de probabilidad es la *normal* (o *gaussiana*), que describe la noci\u00f3n de que las probabilidades de ocurrencia de un evento est\u00e1n concentradas de forma sim\u00e9trica alrededor de un valor central.\n</code></pre> <p>y resulta en:</p> <p>La m\u00e1s famosa de las funciones de probabilidad es la normal (o gaussiana), que describe la noci\u00f3n de que las probabilidades de ocurrencia de un evento est\u00e1n concentradas de forma sim\u00e9trica alrededor de un valor central.</p> <p>El string <code>\"\"</code> le quita el t\u00edtulo.</p>"},{"location":"transcripcion/#ejemplos-o-ejercicios","title":"Ejemplos o ejercicios","text":"<p>Los ejemplos (mostrados dentro de <code>\\begin{framex} ... \\end{framex}</code>) son casos especiales del texto que tendr\u00e1n el siguiente formato:</p> LaTeXMarkdown <pre><code>\\begin{framex}{Ejemplo de la divisibilidad por un n\u00famero primo}{Definici\u00f3n cl\u00e1sica de Laplace}\n\n\\begin{planteamiento}\nDetermine la probabilidad de que un n\u00famero natural cualquiera es divisible por un n\u00famero primo $n$.\n\\end{planteamiento}\n\n\\begin{itemize}\n\\item Si $n$ es un n\u00famero primo, entonces cada $n$-\u00e9simo n\u00famero (empezando por $n$) es divisible por $n$.\n\\item Por lo tanto, en $n$ enteros consecutivos hay un resultado favorable, y por tanto:\n\\end{itemize}\n\\[\nP(\\text{un n\u00famero es divisible por un primo $n$}) = \\frac{1}{n}\n\\]\n\n\\begin{figure}\n\\centering\n\\begin{tikzpicture}\n    ...\n\\end{tikzpicture}\n\\end{figure}\n\n\\end{framex}\n</code></pre> <pre><code>    ---\n\n    :material-pencil-box: **EJEMPLO**\n\n    !!! example \"T\u00edtulo representativo del problema\"\n        Texto del enunciado dentro de la *admonici\u00f3n* tipo `example`, que incluye los datos del problema y la pregunta por resolver. \n\n    Aqu\u00ed est\u00e1n \n\n    - todos\n    - los pasos\n    - necesarios\n    - para la soluci\u00f3n\n\n    y finalmente:\n\n    !!! note \"\"\n        Respuesta al problema con \u00e9nfasis dentro de la *admonici\u00f3n*.\n\n    ---\n</code></pre> <p>y resulta en:</p> <p> EJEMPLO</p> <p>T\u00edtulo representativo del problema</p> <p>Texto del enunciado dentro de la admonici\u00f3n tipo <code>example</code>, que incluye los datos del problema y la pregunta por resolver. </p> <p>\u00bfCu\u00e1l es la expresi\u00f3n para la probabilidad del evento \\(A \\cap B\\) en la gr\u00e1fica mostrada?</p> <p></p> <p>Aqu\u00ed est\u00e1n los pasos de la soluci\u00f3n del problema. Incluyendo gr\u00e1ficas:</p> <p></p> <p>y tambi\u00e9n ecuaciones:</p> \\[\\begin{equation} P(A \\cap B) = P(A) P(B \\mid A) \\end{equation}\\] <ul> <li>y todos</li> <li>los pasos</li> <li>necesarios</li> <li>para la soluci\u00f3n</li> </ul> <p>y finalmente:</p> <p>Respuesta al problema con \u00e9nfasis dentro de la admonici\u00f3n tipo <code>note</code> pero sin t\u00edtulo (con <code>\"\"</code>) y mostrando las ecuaciones relevantes:</p> \\[ P(A \\cap B) = P(A) P(B \\mid A) \\] <p>Observaciones</p> <ul> <li>Los ejemplos deben tener un t\u00edtulo representativo</li> <li>Actualmente el enunciado est\u00e1 dentro del entorno <code>planteamiento</code>.</li> <li>Notar la l\u00ednea horizontal <code>---</code> antes y despu\u00e9s del ejemplo</li> <li>Las ecuaciones en bloque dentro de las admoniciones deben estar rodeadas por <code>$$</code>.</li> <li>Respetar la estructura:</li> </ul> <pre><code>---\n\n:material-pencil-box: **EJEMPLO**\n\n!!! example \"T\u00edtulo del problema\"\n    Texto del enunciado. \n\nPasos de la soluci\u00f3n.\n\n!!! note \"\"\n    Respuesta al problema.\n\n---\n</code></pre> <p>Nota: puede copiar y llevar el fragmento anterior cada vez que va a incluir un ejemplo.</p>"},{"location":"transcripcion/#uso-de-inteligencia-artificial-generativa","title":"Uso de inteligencia artificial generativa","text":"<p>\u00bfPodemos usar IA?</p> <p>\u00a1Por supuesto!<sup>1</sup> (y recomendado). ChatGPT, Gemini, DeepSeek, Claude o cualquier otro pueden ayudar a transformar el c\u00f3digo de LaTeX en Markdown, solo hay que pedirlo. Pero \u00a1cuidado!, hay que revisar bien la sintaxis y los casos especiales y verificar su presentaci\u00f3n en la documentaci\u00f3n, para que sea fiel (totalmente fiel) a la presentaci\u00f3n, especialmente con precisi\u00f3n de la matem\u00e1tica, etc.</p> <ol> <li> <p>No podr\u00eda evitarlo, de todos modos  \u21a9</p> </li> </ol>"}]}